## Описание ноутбуков

### Lab1.ipynb  
**Тема: Введение в нейронные сети и перцептрон**  
- Теоретический обзор искусственного нейрона: входы, весовые коэффициенты, пороговые функции.  
- Реализация однослойного перцептрона «с нуля» на Python (NumPy): прямой проход, функция активации.  
- Обучение модели методом перцептронного обучения: правило корректировки весов, критерий сходимости.  
- Визуализация границ принятия решений на синтетических двухмерных данных.  

### Lab2.ipynb  
**Тема: Многослойный перцептрон и алгоритм обратного распространения ошибки**  
- Архитектура MLP: скрытые слои, функции активации (sigmoid, ReLU).  
- Вывод формул прямого и обратного прохода; расчёт градиентов методом цепочки.  
- Реализация backpropagation «с нуля» на NumPy: настройка learning rate, количества эпох и батчей.  
- Применение `sklearn.neural_network.MLPClassifier` для классификации на примере Iris; подбор гиперпараметров через GridSearchCV.  

### Lab3.ipynb  
**Тема: Построение нейросетей с помощью Keras/TensorFlow**  
- Установка и конфигурация среды TensorFlow 2.x, базовый обзор API Keras.  
- Загрузка и подготовка данных (MNIST, Fashion MNIST): нормализация, one-hot кодирование меток.  
- Создание `Sequential`-модели: слои `Flatten`, `Dense`, активации и компиляция (`optimizer`, `loss`, `metrics`).  
- Обучение и оценка модели: визуализация кривых loss/accuracy, отображение confusion matrix.  

### Lab4.ipynb  
**Тема: Сверточные нейронные сети (CNN) для задач компьютерного зрения**  
- Основы сверточного слоя (`Conv2D`), пулинга (`MaxPooling2D`), padding, stride.  
- Построение простой CNN для классификации изображений (CIFAR-10 или MNIST): несколько свёрток, пулов и полносвязных слоёв.  
- Регуляризация: Dropout, BatchNormalization, ранняя остановка (EarlyStopping).  
- Data augmentation через `ImageDataGenerator`; визуализация feature maps и фильтров.  

### Lab5.ipynb  
**Тема: Рекуррентные сети (RNN), автоэнкодеры и GAN**  
- Основы RNN: последовательная обработка, проблемы затухающих/взрывающихся градиентов; архитектуры LSTM и GRU.  
- Применение LSTM/GRU для задач предсказания временных рядов или генерации текста.  
- Автоэнкодер: кодировщик-декодировщик, снижение размерности, восстановление входных данных.  
- Краткое введение в вариационные автоэнкодеры (VAE) или генеративно-состязательные сети (GAN): структура и принципы обучения.  
